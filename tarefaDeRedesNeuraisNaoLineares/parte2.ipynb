{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward(xamostra):\n",
    "    j = 0\n",
    "    for W in pesos_da_rede:  # Suponhamos que pesos_da_rede seja uma lista de matrizes de peso\n",
    "        if j == 0:\n",
    "            i = np.dot(W, xamostra)\n",
    "            y = g(i)\n",
    "        else:\n",
    "            ybias = np.insert(y, 0, -1)  # Adiciona -1 na primeira posição do vetor y\n",
    "            i = np.dot(W, ybias)\n",
    "            y = g(i)\n",
    "        j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackWard(xamostra, d):\n",
    "    j = len(pesos_da_rede) - 1\n",
    "    while j >= 0:\n",
    "        if j + 1 == len(pesos_da_rede):\n",
    "            delta = g_prime(i[j]) * (d - y[j])\n",
    "            if j > 0:\n",
    "                ybias = np.insert(y[j - 1], 0, -1)\n",
    "                pesos_da_rede[j] += learning_rate * np.outer(delta, ybias)\n",
    "            else:\n",
    "                pesos_da_rede[j] += learning_rate * np.outer(delta, xamostra)\n",
    "        else:\n",
    "            Wb = np.transpose(pesos_da_rede[j + 1])[:, 1:]  # Remove a coluna que multiplica pelos limiares de ativação\n",
    "            delta = g_prime(i[j]) * np.dot(Wb, delta)\n",
    "            ybias = np.insert(y[j - 1], 0, -1)\n",
    "            pesos_da_rede[j] += learning_rate * np.outer(delta, ybias)\n",
    "        j -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_EQM(Xtreino):\n",
    "    EQM = 0.0\n",
    "    for amostra in Xtreino:\n",
    "        xamostra = amostra[0]  # Suponhamos que a amostra seja uma lista onde o primeiro elemento é a entrada\n",
    "        d = amostra[1]  # O segundo elemento é o rótulo\n",
    "\n",
    "        EQI = 0.0\n",
    "        Forward(xamostra)\n",
    "\n",
    "        for j in range(len(d)):\n",
    "            EQI += (d[j] - y[-1][j]) ** 2  # y[-1] é a saída da camada de saída\n",
    "\n",
    "        EQM += EQI\n",
    "\n",
    "    EQM /= (2 * len(Xtreino))\n",
    "    return EQM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(Xtreino, CritérioParada, MaxEpoch):\n",
    "    EQM = 1.0\n",
    "    Epoch = 0\n",
    "\n",
    "    while EQM > CritérioParada and Epoch < MaxEpoch:\n",
    "        for amostra in Xtreino:\n",
    "            xamostra = amostra[0]  # Suponhamos que a amostra seja uma lista onde o primeiro elemento é a entrada\n",
    "            d = amostra[1]  # O segundo elemento é o rótulo\n",
    "\n",
    "            # Chamando as funções Forward e BackWard (substitua por seus próprios métodos)\n",
    "            Forward(xamostra)\n",
    "            BackWard(xamostra, d)\n",
    "\n",
    "        EQM = calcula_EQM()  # Suponhamos que você tenha uma função para calcular o erro médio quadrático\n",
    "        Epoch += 1\n",
    "\n",
    "# Exemplo de uso\n",
    "Xtreino = [(entrada1, rotulo1), (entrada2, rotulo2), ...]  # Substitua pelas suas amostras de treinamento\n",
    "CritérioParada = 0.001  # Defina o critério de parada desejado\n",
    "MaxEpoch = 1000  # Defina o número máximo de épocas\n",
    "\n",
    "treinamento_rede_neural(Xtreino, CritérioParada, MaxEpoch)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
